<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>AutoGraph: Predicting Lane Graphs from Traffic</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://jzuern.github.io/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="http://trackletmapper.cs.uni-freiburg.de">
            TrackletMapper
          </a>
          <a class="navbar-item" href="http://urbanlanegraph.cs.uni-freiburg.de">
            UrbanLaneGraph
          </a>
          <a class="navbar-item" href="http://av-vehicles.cs.uni-freiburg.de/">
            AVDet
          </a>
          <a class="navbar-item" href="http://deepterrain.cs.uni-freiburg.de/">
            DeepTerrain
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">AutoGraph: Predicting Lane Graphs from Traffic</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://jzuern.github.io/">Jannik ZÃ¼rn</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://eng.ox.ac.uk/people/ingmar-posner/">Ingmar Posner</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://www.utn.de/person/wolfram-burgard/">Wolfram Burgard</a><sup>3</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Freiburg, Germany,</span>
            <span class="author-block"><sup>2</sup>University of Oxford,</span>
            <span class="author-block"><sup>3</sup>University of Technology Nuremberg</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2306.15410"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2306.15410"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!--              &lt;!&ndash; Video Link. &ndash;&gt;-->
              <!--              <span class="link-block">-->
              <!--                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"-->
              <!--                   class="external-link button is-normal is-rounded is-dark">-->
              <!--                  <span class="icon">-->
              <!--                      <i class="fab fa-youtube"></i>-->
              <!--                  </span>-->
              <!--                  <span>Video</span>-->
              <!--                </a>-->
              <!--              </span>-->
              <!-- Code Link. -->
              <!--              <span class="link-block">-->
              <!--                <a href="https://github.com/google/nerfies"-->
              <!--                   class="external-link button is-normal is-rounded is-dark">-->
              <!--                  <span class="icon">-->
              <!--                      <i class="fab fa-github"></i>-->
              <!--                  </span>-->
              <!--                  <span>Code</span>-->
              <!--                  </a>-->
              <!--              </span>-->
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="http://urbanlanegraph.cs.uni-freiburg.de"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Dataset</span>
                  </a>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- <video id="teaser" autoplay muted loop playsinline height="100%">
      <source src=""
              type="video/mp4">
    </video> -->

      <img src="static/images/covergirl-autograph-new.png" />

      <h2 class="subtitle has-text-centered">
        AutoGraph aggregates tracked vehicle
        tracklets and predicts complex lane graphs without requiring
        any lane graph annotations.
      </h2>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->

    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Lane graph estimation is a long-standing problem
            in the context of autonomous driving in urban environments.
            Previous works aimed at solving this problem by relying on
            large-scale human-annotated graph annotations, introducing
            the bottleneck of limited available annotations for training
            models to solve this task. To overcome this limitation, we
            propose a novel data source for lane graph annotations: the
            movement of traffic participants.        </p>
          <p>
            In our AutoGraph approach,
            we employ a pre-trained object tracker and collect the tracklets
            of traffic participants such as vehicles and trucks. We show that
            it is possible to train a successor lane graph prediction model
            from this data without requiring any human supervision. In
            the second stage, we show how the single successor predictions can be aggregated into an accurate and consistent lane
            graph. We demonstrate the efficacy of our approach on the
            UrbanLaneGraph dataset and perform extensive quantitative
            and qualitative evaluations.
          </p>
        </div>
      </div>
    </div>


  </div>
  <!--/ Abstract. -->

  <!--    &lt;!&ndash; Paper video. &ndash;&gt;-->
  <!--    <div class="columns is-centered has-text-centered">-->
  <!--      <div class="column is-four-fifths">-->
  <!--        <h2 class="title is-3">Video</h2>-->
  <!--        <div class="publication-video">-->
  <!--          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"-->
  <!--                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>-->
  <!--        </div>-->
  <!--      </div>-->
  <!--    </div>-->
  <!--    &lt;!&ndash;/ Paper video. &ndash;&gt;-->

  </div>
</section>






<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Approach</h2>

        <p>
          Our approach can be split into three distinct stages: First,
          tracklet parsing and merging, where we track traffic participants through all scenes in the dataset and prepare the data
          for model training. Second, model training, where we train
          the proposed models with data obtained in the first stage.
          Third, we perform inference with our trained models and
          aggregate the graphs into a globally consistent representation.
          In the following, we detail each component of our approach. <br><br>
        </p>


        <h3 class="title is-4">Tracklet Parsing and Merging</h3>
        <p>
          We start our data processing pipeline by tracking traffic participants in all available scenes of the Argoverse2
          dataset across all six available cities. Each scene in the dataset consists of approximately 20 seconds of driving.
          For each scene, we track vehicles such as cars, trucks, motorcycles, and busses using a pre-trained LiDAR-based
          object detector. We transform all tracklets into a global
          coordinate frame. Subsequently, we smooth the tracklets with
          a moving average filter in order to minimize the amount
          of observation noise and the influence of erradic driving
          behavior (i.e. steering inaccuracies).
          <br><br>




        </p>
        <div class="content has-text-justified has-text-centered"> <img src="static/images/merging.png" width="50%" />




          <h3 class="title is-4">Successor Lane Graph Prediction</h3>
          <p>
            The whole training pipeline is visualized in the figure below. After
            our aggregation step, we are able to query all tracklets that
            are visible in an aerial image crop, starting from a given
            querying position. To obtain a training dataset for our
            models, for each query pose, we crop an aerial image from the aerial image,centered and oriented around
            the query pose. In the same way, we crop and center the
            drivable map and the angle map.<br><br>

            Our model consists of two sub-networks. As a first step,
            we train a DeepLabv3+ model to predict the pixel-wise
            drivable and angle maps from an RGB aerial image input. We denote this
            model as TrackletNet. This initial task is identified as an
            auxiliary task, leveraging the vast amount of tracklets readily
            available for a given crop. For training, we use a binary
            cross-entropy loss to guide the prediction of the drivable map
            layer and a mean squared error loss for the prediction of
            the angle map. <br>


            In the second step, we train a separate DeepLabv3+
            model to predict the successor graph from a certain pose q,
            which we parameterize as a heatmap. To account for
            the additional Drivable and Angles input layers, we adapt
            the number of input layers of the DeepLabv3+ model architecture. We denote this model as SuccessorNet. To obtain
            per-pixel labeling of the successor graph in the image crop,
            we render the successor graph as a heatmap in the crop by
            drawing along the graph edges with a certain stroke width.
            This heatmap highlights all regions in the aerial image that
            are reachable by an agent placed at pose q. We train our
            SuccessorNet model with a binary cross-entropy loss.
            Finally, we skeletonize the predicted heatmap using a
            morphological skinning process and convert the skeleton
            into a graph representation.



          </p>
          <div class="content has-text-justified has-text-centered"> <img src="static/images/approach.png" width="100%" />
          </div>
          <h3 class="title is-4"> Graph Exploration and Aggregation</h3>
          <p>
            In this section, we illustrate how
            a complete lane graph can be obtained from running our
            AutoGraph model iteratively on its own predictions and
            subsequently aggregating these predictions into a globally
            consistent graph representation. To this end, we leverage a
            depth-first exploration algorithm: We initialize our model by
            selecting start poses, which can either be selected manually
            or obtained from our TrackletNet model. We predict the
            successor graph from this initial position and query our
            model along the successor graph and repeat the process. In
            the case of a straight road section, for each forward pass
            of our model, a single future query pose is added to the
            list of query poses to process. If a lane split is encountered,
            for each of the successor subgraphs starting at lane splits, a
            query pose is added to the list. If a lane ends or no successor
            graphs are found, the respective branch of the aggregated
            lane graph terminates and the next pose in the list is queried.
            The exploration terminates once the list of future query poses
            is empty.  <br><br>

          </p>
        </div>
      </div>
    </div>
</section>




<section class="hero teaser">
  <div class="container is-two-fifths-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline width="100%">
        <source src="./static/images/out.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle is-two-fifths-desktop has-text-centered">
        AutoGraph learns to predict successor graphs from vehicle tracklets and aggregates them into a single consistent lane graph
      </h2>
    </div>
  </div>
</section>



<section class="section" >
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">UrbanTracklets Dataset</h2>
        <div class="content has-text-justified has-text-centered"> <img src="static/images/tracklets.png" /></div>
        <p>
          We evaluate our proposed method on a large-scale dataset
          for lane graph estimation from traffic participants. We use
          the RGB aerial images and the ground-truth lane graph
          annotations from the UrbanLaneGraph dataset. To obtain
          the traffic participant tracklets, we leverage the LiDAR
          dataset split of the Argoverse2 dataset. The dataset
          contains consecutive LiDAR scans for hundreds of driving
          scenarios. We track the vehicle classes of Car, Bus, Trailer,
          and Motorcycle. Subsequently, we transform the respective
          LiDAR-centric tracklet coordinates to a global reference
          frame that is aligned with the aerial image coordinates.
          We smooth each tracklet with a mean filter approach to
          account for sensor noise and tracking inaccuracies. We call
          our tracklet dataset the UrbanTracklet dataset and make it
          publicly available as an addition to the UrbanLaneGraph
          dataset. Below, we list all relevant metrics of our UrbanTracklet dataset.
          In total, our dataset entails tracklets with
          an accumulated total length of approximately 12 000 km. <br><br>
        </p>

        <div class="content"> <img src="static/images/dataset-stats.png" width="50%" name="dataset"/>

          <h3 class="title is-3">Dataset Download</h3>

          <p>
            We make our UrbanTracklet dataset available for download here:
          </p>


          <span class="link-block">
                <a href="http://aisdatasets.informatik.uni-freiburg.de/autograph/trajectories.zip"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Dataset</span>
                </a>
          </span>

          <p>
            <br>
            After unzipping, the dataset contains a .npy file for each city in the UrbanLaneGraph dataset. Each .npy
            file contains a list of tracklets for the respective city. The respective aerial images and human-annotated
            lane graphs may be found in the <a href="http://urbanlanegraph.cs.uni-freiburg.de/" target="_blank">UrbanLaneGraph dataset</a>.
          </p>






        </div>
      </div>
    </div>
  </div>
</section>







<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="columns is-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Experiments</h2>



          We evaluate our model on well-established tasks for
          lane graph estimation: Successor Lane Graph Prediction and
          Full Lane Graph Prediction. <br> <br>

                    <h3 class="title is-4"> Successor Lane Graph Prediction</h3>


          Below, we visualize qualitative results of our AutoGraph and AutoGraph-GT models for the Successor-LGP task on the UrbanLaneGraph dataset.

          We observe that overall both models are capable of modeling the multimodal nature of successor
          graphs efficiently, however, the
          AutoGraph-GT model shows slightly better-defined heatmap
          outputs, since the annotations used for training were created
          from the ground-truth successor lane graph. For details, please check out the paper. <br><br>



          <div class="is-vcentered interpolation-panel">
            <!-- <p>Sequence 1</p> -->
            <div class="has-text-centered">
              <img width="100%" src="static/images/results.png"> </div>
            <p>Qualitative results of our AutoGraph model for the Successor-LGP task on the UrbanLaneGraph dataset. We visualize
              the successor heatmap and the graph generated from it for our human-supervised model AutoGraph-GT and our trackletsupervised model AutoGraph.</p>
            <hr>
          </div>

          <h3 class="title is-4">Full Lane Graph Prediction</h3>


          <p>Below, we illustrate two exemplary visualizations of
            predicted lane graphs for the cities of Washington, D.C., and
            Miami. <br> <br>
            We observe that our approach is capable of accurately
            reconstructing the lane graph in visually challenging environments. Large scenes with multiple blocks are handled well
            and clearly reflect the underlying lane graph topology. The
            detail view for a complex intersection in Miami illustrates that almost
            all major intersection arms are covered even in
            the presence of visual clutter such as water, boats, parking
            lots, and concrete-colored buildings. Minor inaccuracies are
            produced at the five-armed intersection at the bottom of the
            aerial image, where not all connections between intersection
            arms are present in the inferred lane graph. <br><br>

          </p>

          <div class="is-vcentered interpolation-panel">
            <!-- <p>Sequence 1</p> -->
            <div class="has-text-centered">
              <img width="100%" src="static/images/washington.png"> </div>
            <p>Full lane graph prediction result - Washington, D.C.</p>
            <hr>
            <!-- <p>Sequence 2</p> -->
            <div class="has-text-centered">
              <img width="100%" src="static/images/miami.png"> </div>
            <p>Full lane graph prediction result - Miami, detail view.</p>
          </div>
        </div>

        <hr>
        <!-- <p>Sequence 3</p> -->
      </div>
      <hr>
    </div>
  </div>
</section>









<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{zurn2023autograph,
  title={AutoGraph: Predicting Lane Graphs from Traffic Observations},
  author={Z{\"u}rn, Jannik and Posner, Ingmar and Burgard, Wolfram},
  journal={arXiv preprint arXiv:2306.15410},
  year={2023}
}</code></pre>
  </div>
</section>




<!-- Concurrent Work. -->
<section class="section" id="ack">
  <div class="container is-max-desktop content">

    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Acknowledgements</h2>

        <div class="content has-text-justified">
          We thank the Argoverse2 team for making the Argoverse2 datset (<a href="https://www.argoverse.org/av2.html">https://www.argoverse.org/av2.html</a>)
          publicly available and for allowing the re-distribution of their dataset in remixed form.
        </div>
      </div>
    </div>
  </div>
</section>





<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">People</h2>
    <div class="columns container">
      <div class="column has-text-centered profile">
        <a href="https://jzuern.github.io/"><img src="static/images/people/zuern.jpg" alt="Jannik ZÃ¼rn" /></a>
        <h3><a href="https://jzuern.github.io//">Jannik ZÃ¼rn</a></h3>
      </div>
      <div class="column has-text-centered profile">
        <a href="https://eng.ox.ac.uk/people/ingmar-posner"><img src="static/images/people/posner.jpg" alt="Ingmar Posner" /></a>
        <h3><a href="https://eng.ox.ac.uk/people/ingmar-posner">Ingmar Posner</a></h3>
      </div>
      <div class="column has-text-centered profile">
        <a href="https://www.utn.de/1/wolfram-burgard/"><img src="static/images/people/burgard.jpg" alt="Wolfram Burgard" /></a>
        <h3><a href="https://www.utn.de/1/wolfram-burgard/">Wolfram Burgard</a></h3>
      </div>
    </div>
  </div>
</section>



</body>
</html>
